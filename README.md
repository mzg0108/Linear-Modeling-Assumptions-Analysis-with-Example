The Jupyter notebook file named "linear_regression_analysis" analysis the 5 assumptions of linear modeling. 
Here are the 5 assumptions of linear modeling:
**
1.    Linearity (Linear Relationship)

2.    Independence of Errors (No Autocorrelation)

3.    Homoscedasticity (Constant Variance of Residuals)

4.    Normality of Residuals (Normal Distribution of Errors)

5.    No Multicollinearity (Independence of Predictors)**


 
**Explanation for linear modeling assumptions with examples 
**
**1. Linearity (Straight Line)**

    Definition: The relationship between two things follows a straight line. If one goes up, the other goes up (or down) steadily.

    Example: The more hours you study, the higher your test score gets. Itâ€™s a straight path up!
   
**3. Independence of Errors (No Copying)**

    Definition: Each mistake or data point is separate and doesn't copy the one before it.

    Example: Flipping a coin. Just because you got "Heads" last time doesn't mean you will get "Heads" this time. They are totally separate.

**3. Homoscedasticity (Same Spread)**

    Definition: The size of your "mistakes" stays the same, no matter how big the numbers get.

    Example: Imagine trying to guess how many jellybeans are in a jar. To follow this rule, you should be just as good at guessing for a small jar as you are for a giant jar. You shouldn't get worse just because the jar is bigger.

**4. Normality of Errors (The Bell Curve)**

    Definition: Most of the time, the model is only a little bit wrong. Being very wrong is rare.

    Example: If you throw paper balls at a trash can, most will land close to the can. Very few will hit the opposite wall.

**5. No Multicollinearity (No Double Clues)**

    Definition: You shouldn't use two clues that tell you the exact same thing.

    Example: If you want to guess how tall someone is, you don't need to measure both their "left shoe" and their "right shoe." They are basically the same size! You only need one.



